{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac83f13",
   "metadata": {},
   "source": [
    "# ***Footnotes_Assassin*** #\n",
    "`v3.0.0`\n",
    "\n",
    "### Purpose ###\n",
    "\n",
    "A tool to improve the text-to-speech experience for University, Grad School, etc., by giving them a way to bulk remove footnotes, endnotes, headers and any *marginalia* from academic pdfs, which are intended to be consumed via text-to-speech/ read-aloud apps. \n",
    "\n",
    "While this tool is *not* a text-to-speech app, its primary intent is to give users a way to remove unwanted text from pdfs so those pdfs can be ingested to text-to-speech apps and consumed via audio - enabling a more efficient way to turn \"dead time\" (*e.g., driving, chores, other monotonous tasks*) into study/\"reading\" time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31c8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964048cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# footnotes_assassin_v3.0.0 functions #\n",
    "\n",
    "def parse_page_ranges(page_spec):\n",
    "    \"\"\"\n",
    "    Parse a page specification string like \"1-3,5,8-10\" into a set of 0-based page indices.\n",
    "    \"\"\"\n",
    "    pages = set()\n",
    "    if not page_spec:\n",
    "        return pages\n",
    "    parts = page_spec.split(',')\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if '-' in part:\n",
    "            start, end = part.split('-')\n",
    "            start, end = int(start), int(end)\n",
    "            pages.update(range(start - 1, end))  # convert to 0-based\n",
    "        else:\n",
    "            pages.add(int(part) - 1)  # single page, 0-based\n",
    "    return pages\n",
    "\n",
    "def extract_text_without_footnotes(\n",
    "    pdf_path\n",
    "    , threshold_ratio=0.90\n",
    "    , header_threshold=50\n",
    "    , pages_to_skip=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, removing:\n",
    "    - Footnotes based on font height (robust mode/median strategy)\n",
    "    - Headers within top X points of the page\n",
    "    - Entirely skipping user-specified pages\n",
    "\n",
    "    Returns:\n",
    "        str: Document text without footnotes/headers/skipped pages\n",
    "    \"\"\"\n",
    "    all_pages_text = []\n",
    "    skip_set = parse_page_ranges(pages_to_skip)\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages):\n",
    "            if page_number in skip_set:\n",
    "                continue\n",
    "\n",
    "            words = page.extract_words()\n",
    "            if not words:\n",
    "                continue\n",
    "\n",
    "            df = pd.DataFrame(words)\n",
    "\n",
    "            # Remove headers if header_threshold is set\n",
    "            if header_threshold is not None:\n",
    "                df = df[df['top'] > header_threshold]\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # --- Robust determination of main text height ---\n",
    "            heights = df['height'].tolist()\n",
    "            heights_sorted = sorted(heights)\n",
    "\n",
    "            # Drop the smallest 25% of heights to avoid bias toward footnotes\n",
    "            cutoff_index = int(len(heights_sorted) * 0.25)\n",
    "            filtered_heights = heights_sorted[cutoff_index:] if cutoff_index < len(heights_sorted) else heights_sorted\n",
    "\n",
    "            main_height = None\n",
    "            if filtered_heights:\n",
    "                height_counts = Counter(filtered_heights)\n",
    "                main_height = height_counts.most_common(1)[0][0]\n",
    "\n",
    "            # Fallback to median if needed\n",
    "            if main_height is None:\n",
    "                main_height = statistics.median(heights)\n",
    "\n",
    "            # --- Define threshold and filter ---\n",
    "            height_threshold = main_height * threshold_ratio\n",
    "            filtered_words = df[df['height'] >= height_threshold]\n",
    "\n",
    "            # Sort and reconstruct text\n",
    "            filtered_words = filtered_words.sort_values(by=['top', 'x0'])\n",
    "            page_text = \" \".join(filtered_words['text'].tolist())\n",
    "\n",
    "            all_pages_text.append(page_text)\n",
    "\n",
    "    return \"\\n\\n\".join(all_pages_text)\n",
    "\n",
    "\n",
    "def pdf_to_txt(\n",
    "    pdf_path\n",
    "    , txt_output_path=None\n",
    "    , output_dir=None\n",
    "    , threshold_ratio=0.90\n",
    "    , header_threshold=50\n",
    "    , pages_to_skip=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts text and writes it to a .txt file.\n",
    "    If txt_output_path is not provided, automatically uses:\n",
    "        '{'pdf_basename'}'_CLEANED_TEXT.txt\n",
    "    If output_dir is provided, the file is saved there (created if missing).\n",
    "    Otherwise, it's saved in the same directory as the PDF.\n",
    "    \"\"\"\n",
    "    # Determine output filename\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_filename = f\"{base_name}_CLEANED_TEXT.txt\"\n",
    "\n",
    "    # Decide where to place it\n",
    "    if txt_output_path:\n",
    "        final_path = txt_output_path\n",
    "    else:\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            final_path = os.path.join(output_dir, output_filename)\n",
    "        else:\n",
    "            dir_name = os.path.dirname(pdf_path)\n",
    "            final_path = os.path.join(dir_name, output_filename)\n",
    "\n",
    "    text = extract_text_without_footnotes(\n",
    "        pdf_path=pdf_path\n",
    "        , threshold_ratio=threshold_ratio\n",
    "        , header_threshold=header_threshold\n",
    "        , pages_to_skip=pages_to_skip\n",
    "    )\n",
    "\n",
    "    with open(final_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"Cleaned text exported to: {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858258d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main intended usage - ingest single pdf from input folder, export to output folder;\n",
    "\n",
    "pdf_to_txt(\n",
    "    pdf_path=\"./input/sample_article.pdf\",\n",
    "    output_dir=\"./output/\",\n",
    "    threshold_ratio=0.90,\n",
    "    header_threshold=50,\n",
    "    pages_to_skip=\"1, 3, 14\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXAMPLE USAGE:\n",
    "# Example 1 — Auto filename in same directory as PDF\n",
    "pdf_to_txt(\n",
    "    pdf_path=\"./input/My_Research_Article.pdf\"\n",
    "    , threshold_ratio=0.88\n",
    "    , header_threshold=50\n",
    "    , pages_to_skip=\"1-2, 10-12\"\n",
    ")\n",
    "\n",
    "# Example 2 — Send all outputs to a dedicated folder\n",
    "pdf_to_txt(\n",
    "    pdf_path=\"./input/Another_Paper.pdf\"\n",
    "    , output_dir=\"./output\"\n",
    "    , threshold_ratio=0.90\n",
    "    , header_threshold=50\n",
    "    , pages_to_skip=\"1\"\n",
    ")\n",
    "\n",
    "# Example 3 — Batch process multiple PDFs into one output_dir\n",
    "pdfs = [\n",
    "    \"./input/article1.pdf\"\n",
    "    , \"./input/article2.pdf\"\n",
    "    , \"./input/article3.pdf\"\n",
    "]\n",
    "\n",
    "for p in pdfs:\n",
    "    pdf_to_txt(\n",
    "        pdf_path=p\n",
    "        , output_dir=\"./output\"\n",
    "        , threshold_ratio=0.88\n",
    "        , header_threshold=50\n",
    "        , pages_to_skip=\"1-2\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecc3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3609bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e761c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
